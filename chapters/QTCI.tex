\chapter{Quantics Tensor Cross Interpolation}
\label{chap:2}
The widespread and increasing interest in tensor network methodologies - with explicit focus on \textit{matrix product states} (MPSs) \cite{Fannes1992} - outside of the many-body community has facilitated the development of a pivotal\footnote{A rather playful wording choice given the context, as it will become clearer in the subsequent sections.} extension to the tensor network set of tools: the Tensor Cross Interpolation (TCI) algorithm.


\section{The algorithm}

\begin{definition}[Compressible Tensor]
	A tensor $\mathcal{T}$ is {\normalfont \textbf{compressible}} or {\normalfont \textbf{low-rank}} if it can be approximated by a Matrix Product State with small rank $\chi$.
\end{definition}

\note{Introduction}{Sign problem-free integration, \textit{superhigh-resolution}, exponential reductions in computational costs (\textit{curse of dimensionality}) $\to$ low-rank = calculations in polynomial time with no truncation (common approach), uncover hidden structures}\\

\noindent\note{Other methods}{High dimensional integrals and exponential complexity of QMB systems, Quantum Montecarlo shortcomings: sign problem and slow convergence ($1 / \sqrt{N}$), different from TN for wave function variational ansatz (DMRG)}\vspace{\baselineskip}\\

\subsection{Matrix Cross Interpolation and prrLU}
\note{Matrix Cross Interpolation}{Main paper: \cite{Goreinov1997, Schneider2010}, Error is at most $O(\chi^2)$ the optimal error of SVD, CI vs prrLU, exact for the selected rows and columns (\textit{interpolation}) $A(\mathcal{I},\mathcal{J})*A(\mathcal{I},\mathcal{J})^{-1}A(\mathcal{I},\mathds{J}) = A(\mathcal{I},\mathds{J})  $ and for $\rank{A} = \chi$, heuristic \textit{maxvol principle} = maximise determinant of the pivot matrix, using continuous MCI already reduces the complexity of integration and derivation, CI is numerically unstable for large $\chi$}\vspace{\baselineskip}\\
\note{Nice Sentences}{The RHS contains only small subparts of the original matrix. }\\

\noindent\todo{Insert a numerical example of CI and prrLU $\to$ use the DFT matrix}


\subsection{Tensor Cross Interpolation}
\note{Tensor Cross Interpolation}{Main paper: \cite{Fernandez2024}, Main TCI ref: \cite{Oseledets2010}, polynomial dimension scaling, matrix cross interpolation \cite{Goreinov1997}, function learning, N-dimensional integrals, \textit{$\epsilon$-factorizability} of continuous functions $\to$ reduction from N to one-dimensional integrals independent of function sign ($\int \text{d}\mathbf{x} f(x_1, ... , x_N) \approx \prod\limits_{\alpha = 1 }^N\int \text{d}x_1 T_\alpha(x_\alpha)P_\alpha^{-1}$) and $|| f - f_\text{TCI} || < \epsilon $, N-dimensional integral $O(Nd\chi^2)$ (not $d^N$) (??) , tensor representation $O(N\chi^2)$ (??), \textit{active machine learning} \cite{Settles2012}: find the region with the largest approximation error, main challenge: bookkeeping and notation, \textit{maxvol} principle \cite{Dolgov2020}, exact if $\rank{TT} = \chi$ like MCI (see Naive Approach \cite{Fernandez2022}), \textit{multi-indices} and \texttt{Vector}\{\textit{multi-indices}\}, $\bigoplus$= concatenation of multi-indices, \textit{nesting condition} \cite{Oseledets2011,Dolgov2020}$\to$ $\mathcal{I}_\alpha \subset \mathcal{I}_{\alpha -1 } \bigoplus \mathds{K}_\alpha$ and $\mathcal{J}_\alpha \subset \mathds{K}_\alpha \bigoplus \mathcal{J}_{\alpha +1 }$(this guarantees exact interpolation for $f$(pivots)), function calls: $O(Nd\chi^2) \ll d^N$, $\Pi_\alpha$ and the \textit{error function} $\epsilon_\Pi = |f - f_\text{TCI}|(i, x_\alpha, x_{\alpha +1}, j)$ $\to$ find the maximum of $\epsilon_\Pi$ before adding new pivots (add the pivots that yields the largest improvement of the local accuracy), pivot search: \textit{full}, \textit{rook}, \textit{block-rook}, \textit{alternate}(alternate:single pivot $O(d\chi)$, total search $O(d\chi^2)$ \cite{Fernandez2022}), \textit{environment-aware} $\epsilon_\Pi$ $\to$ error function related to the error of the integral that takes in consideration the volume contribution to the integral, \textit{environment} vectors $L_i$ and $R_j$, \textit{rank-revealing} (only slow convergence for high rank), \textit{polynomial costs} }\\

\noindent \todo{Insert Algorithm description (see below)}
\begin{algorithm}
\caption{An algorithm with caption}\label{alg:cap}
\begin{algorithmic}
\If{something}
    \State do something else
\EndIf
\end{algorithmic}
\end{algorithm}

\note{Applications}{Feynmann diagramm expansion \cite{Fernandez2022}: high-order real-time nonequilibrium Schwinger-Keldysh perturbation expansions (integral convergence $1/N^2$, $N$ function evaluations), multi-dimensional function minimization and quantized reinforcement learning \cite{Sozykin2022} (outperfoming standardized gradient-free methods in number of function evaluations and execution time), computation of Brilloin zone integrals to calculate topological invariants \cite{Ritter2024}, compact tensorization of atomic orbitals bases with high accuracy \cite{Jolly2024} (error on energy of \ce{H2} improved by 85\% w.r.t. double zeta calculation), speed-up of multi-assets Fourier transform-based option pricing \cite{Sakurai2025}}\\

\noindent\note{Nice sentences}{The approximation is systematically controlled by $\chi$. TCI is a generalization of the matrix cross interpolation to $N$-dimensional tensors and functions. Summation is implicit over the indices connecting two  tensors. TCI representation is defined by the selected $\mathcal{I}$ and $\mathcal{J}$, so an accurate interpolation amounts to optimizing this selection. TCI approximation of $f$ is built from one-dimensional slices  of $f$. Tensor network methods offer a new appraoch to high-dimensional integration. It significantly outperform Monte-Carlo and quasi-Monte-Carlo methods in differente applications. The limiting factor of TCI (rank of the $\epsilon$-factorization) is entirely orthogonal to that of sampling methods. The tensor cross interpolation (TCI) algorithm is a rank-revealing algorithm for decom-
posing low-rank, high-dimensional tensors into tensor trains/matrix product states (MPS).}\\

\noindent\note{General references}{\cite{Oseledets2010, Oseledets2011, Savostyanov2014, Savostyanov2011, Dolgov2020}}\\
\note{Open questions}{Generality of $\epsilon$-factorizability property}

\subsection{The Quantics Representation: QTCI}

\note{QTCI}{Discards weak entanglement between different scales; works well for scale-separated, non-irregular functions; $O(\mathcal{L} \log N )$ ($N \stackrel{?}{=} 2^R $) quantics approximation scaling 4 $\to$ logarithmic scaling in the number of grid points \cite{Khoromskij2011}; exponentially high resolution; low-rank scale separated structure revealed through TCI + quantics }\\

\noindent \note{Nice Sentences}{Numerical computations with multivariate functions typically involve a compromise between two contrary desiderata: accurate resolution of the functional dependence, versus parsimonious memory usage.}\\

\noindent \note{General references}{\cite{Oseledets2009} (quantics and interleaved representation) }\\

\noindent \note{Applications}{Approximation and manipulation of correlation functions for quantum many body systems \cite{Hiroshi2023}; compression of imaginary-time propagators in the Frobenious norm \cite{Takahashi2025}; diagrammatic non-equilibrium many-body Greenâ€™s function-based calculations \cite{Murray2024} $\to$ better scaling of resourses for higher precisions $R$ or longer times }\\

\noindent \note{Open questions 
}{When does a multivariate function admit low-rank representation? }





\section{Computational Fallbacks: Localised Functions}
\subsection{Localised Function and Memory Consumption}


Another cool list:
\begin{itemize}
	\item item 1
	\item item 2\footnote{Some foot note}
	\item item 3
    \item item 3\item item 3\item item 3\item item 3\item item 3\item item 3
\end{itemize}


% \begin{figure}
% 	\centering
% 	\includegraphics[width=.75\textwidth]{example-image-duck}
% 	\caption{Another cool duck}
% 	\label{fig:figure2}
% \end{figure}