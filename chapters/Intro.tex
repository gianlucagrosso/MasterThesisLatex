\chapter{Introduction}
\label{chap:intro}
"Dispelling" the \textit{curse of dimensionality} that "hexes" numerics' computations, has been a quest of prime interest in different fields of science for many years. 
Tensor network techniques -- and in particular \textit{matrix product states} (MPSs) methodologies -- are a well-established \cite{Fannes1992,White1992,Schollwock2011,Vidal2003,VerstraeteCirac2004,vonDelftTNNotes,tensornetwork.org} and standard approach widely employed by the quantum many-body (QMB) physics community to mitigate such exponential increase of computational resources. Numerical simulations of high-dimensional QMB wavefunctions represent, in fact, an exceptionally challenging computational task if not properly addressed.

Meanwhile mathematicians have dedicated, as well, significant effort to target the same problem -- arising from multidimensional tensor approximation of continuous functions and numerical linear algebra computations \cite{Oseledets2009Intro, Kolda2009} -- ultimately converging towards a similar approach: \textit{tensor trains} (TTs)\footnote{From now on \textit{tensor train} and \textit{matrix product state} will be used interchangeably.} \cite{Oseledets2011}. 

The rising interest in tensor-network (TN) methods -- particularly those based on MPSs — has therefore led to the emergence of a standard ``MPS toolbox'' \cite{ttpylib, ITensors.jl, QSpace} and a well-defined catalogue of canonical TN applications \cite{Verstraete2008} (variational ansatz of QMB wavefunctions -- or DMRG -- among the most popular). For a long time, however, techniques for function approximation and manipulation have been laying beyond the traditional TN portfolio.

Although various numerical approximation algorithms have been developed through the years to handle challenges involving function representation and manipulation \cite{Isaacson1994} (integration, convolution, differentiation, etc.), numerous limitations still persist. Conventional approaches are deeply affected by rapid increase of computational capital -- the \textit{curse of dimensionality} -- when aiming to reach high resolution-portrayal of multi-dimensional functions: e.g. exponential resources in memory and computation time are necessary to store and manipulate a naive SVD-based MPS function representation. On the other hand, also standard numerical integration techniques, like Monte Carlo or Quantum Monte Carlo, as stochastic methods, intrisically suffer from slow-convergence scaling w.r.t. dimensionality, together with other computational flaws such as the ``sign problem'' \cite{Loh1990}, making them unfeasable for an always broader and more complex set of problems.  

The widespread and increasing interest in TN and MPSs methodologies among different fields has therefore facilitated the development of a pivotal\footnote{A rather playful wording choice given the context, as it will become clearer in the subsequent sections.} extension to the ``MPS toolbox'' in order to integrate the missing function representation capabilities: the Tensor Cross Interpolation (TCI) algorithm.

TCI attempts to address many of the above-mentioned limitations of standard function approximation and operation algorithms, by revealing low-rank structures and leveraging weakly entangled, scale-separated MPS-based function representations. As a result, they offer significantly improved computational scaling and flexibility, making the (Q)TCI library suite \cite{Fernandez2024} highly attractive for a wide range of advanced applications. Among them the following are definitely worth mentioning (with their respective achievements): high-order real-time nonequilibrium Schwinger-Keldysh perturbation expansions \cite{Fernandez2022} (integral convergence improved from $1/\sqrt{N_{func\_eval}}$ to $1/N_{func\_eval}^2$), multi-dimensional function minimization and quantized reinforcement learning \cite{Sozykin2022} (outperfoming standard gradient-free methods in number of function evaluations and execution time), computation of Brilloin zone integrals for topological invariants evaluation \cite{Ritter2024} (exponential to polynomial-order scaling of integration costs with respect to the simulation parameters), compact tensorization of atomic orbitals bases with high accuracy \cite{Jolly2024} (error on g.s. energy of \ce{H2} improved by 85\% w.r.t. double zeta calculation), (speed-up of) multi-assets Fourier transform-based European option pricing \cite{Sakurai2025}.

More generally the novel approach of Ref. \cite{Fernandez2022, Fernandez2024} to integrate \textit{quantics} tensor rebasing with \textit{tensor cross approximation} procedures -- i.e. Quantics Tensor Cross Interpolation (QTCI) -- opened up the possibility to features like \textit{super-high resolution} and \textit{sign problem-free integration} for multi-dimensional functions, while maintaining computational costs bounded to a polynomial increase. Although the traditional ``TT-toolset'' already allowed for a similar scaling in resources with truncation-based procedures to reduce tensor's rank, (Q)TCI progressively reveals the rank structure of the input tensor by adaptively increasing the number of tensor evaluations (more on this in \prettyref{chap:QTCI}) without any loss of information caused by truncation. For this reason \todo{Machine learning part}

However... \todo{ Here you talk about TCI limitations and patching}


\todo{Here you talk about  content of each chapter }

\note{Tensor Networks}{\textit{MPS or TT toolset} for reducing the rank of tensor object}

\note{Introduction}{Sign problem-free integration, \textit{superhigh-resolution}, exponential reductions in computational costs (\textit{curse of dimensionality}) $\to$ low-rank = calculations in polynomial time with no truncation (common approach), uncover hidden structures}\\

\noindent\note{Other methods}{High dimensional integrals and exponential complexity of QMB systems, Quantum Montecarlo shortcomings: sign problem and slow convergence ($1 / \sqrt{N}$), different from TN for wave function variational ansatz (DMRG)}\vspace{\baselineskip}\\


\textsc{Small caps text}
{\HUGE{Huge text}}


Some different type of text:\hfill

Mixing \textbf{different series, \textsf{families}} and
\textsl{\texttt{shapes,}} \textsc{especially in one sentence,}
is usually \emph{highly inadvisable!}\textit{ it = emph}

Some modification 

\MakeTextUppercase{⟨text⟩}


% \begin{figure}
% 	\centering
% 	\includegraphics[width=.5\textwidth]{example-image-duck}
% 	\caption{A cool duck}
% 	\label{fig:figure1}
% \end{figure}



% \label{sec:outline}

% A cool list:
% \begin{enumerate}
% 	\item item 1
% 	\item item 2
% 	\item item 3
% \end{enumerate}

% \rem{Something}
%  \todo{I have to fix this}
%  \note{Note }{just a note}

% \subsubsection{Some section}
% Some text
% \begin{definition}[Some law]
%     Some details
% \end{definition}
% Thank you for the space
% \change{\note{Gianluca}{I have to do this}}

% \begin{mydef}[Title]
%     Some text
% \end{mydef}
    

% $\Tr (\Gamma)$







